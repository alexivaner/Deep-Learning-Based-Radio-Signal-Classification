{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import os,random\n",
    "from tensorflow.keras.layers import Input,Reshape,ZeroPadding2D,Conv2D,Dropout,Flatten,Dense,Activation,MaxPooling2D,AlphaDropout\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow.keras.models as Model\n",
    "from tensorflow.keras.regularizers import *\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hutom\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\ipykernel_launcher.py:14: H5pyDeprecationWarning: The default file mode will change to 'r' (read-only) in h5py 3.0. To suppress this warning, pass the mode you need to h5py.File(), or set the global default h5.get_config().default_file_mode, or set the environment variable H5PY_DEFAULT_READONLY=1. Available modes are: 'r', 'r+', 'w', 'w-'/'x', 'a'. See the docs for details.\n",
      "  \n",
      "'free' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hutom\\Downloads\\Compressed\\2018.01\\ExtractDataset\\part1.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'free' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hutom\\Downloads\\Compressed\\2018.01\\ExtractDataset\\part2.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'free' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hutom\\Downloads\\Compressed\\2018.01\\ExtractDataset\\part3.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'free' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hutom\\Downloads\\Compressed\\2018.01\\ExtractDataset\\part4.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'free' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hutom\\Downloads\\Compressed\\2018.01\\ExtractDataset\\part5.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'free' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hutom\\Downloads\\Compressed\\2018.01\\ExtractDataset\\part6.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'free' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hutom\\Downloads\\Compressed\\2018.01\\ExtractDataset\\part7.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'free' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hutom\\Downloads\\Compressed\\2018.01\\ExtractDataset\\part8.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'free' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hutom\\Downloads\\Compressed\\2018.01\\ExtractDataset\\part9.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'free' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hutom\\Downloads\\Compressed\\2018.01\\ExtractDataset\\part10.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'free' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hutom\\Downloads\\Compressed\\2018.01\\ExtractDataset\\part11.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'free' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hutom\\Downloads\\Compressed\\2018.01\\ExtractDataset\\part12.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'free' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hutom\\Downloads\\Compressed\\2018.01\\ExtractDataset\\part13.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'free' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hutom\\Downloads\\Compressed\\2018.01\\ExtractDataset\\part14.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'free' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hutom\\Downloads\\Compressed\\2018.01\\ExtractDataset\\part15.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'free' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hutom\\Downloads\\Compressed\\2018.01\\ExtractDataset\\part16.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'free' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hutom\\Downloads\\Compressed\\2018.01\\ExtractDataset\\part17.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'free' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hutom\\Downloads\\Compressed\\2018.01\\ExtractDataset\\part18.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'free' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hutom\\Downloads\\Compressed\\2018.01\\ExtractDataset\\part19.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'free' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hutom\\Downloads\\Compressed\\2018.01\\ExtractDataset\\part20.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'free' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hutom\\Downloads\\Compressed\\2018.01\\ExtractDataset\\part21.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'free' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hutom\\Downloads\\Compressed\\2018.01\\ExtractDataset\\part22.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'free' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hutom\\Downloads\\Compressed\\2018.01\\ExtractDataset\\part23.h5\n",
      "X-size： (1440000, 1024, 2)\n",
      "Y-size： (1440000, 24)\n",
      "Z-size： (1440000, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "f = h5py.File(r'C:\\Users\\hutom\\Downloads\\Compressed\\2018.01\\ExtractDataset\\part0.h5')\n",
    "sample_num = f['X'].shape[0]\n",
    "idx = np.random.choice(range(0,sample_num),size=60000)\n",
    "# idx = np.random.choice(range(0,sample_num),size=30000)\n",
    "X = f['X'][:][idx]\n",
    "Y = f['Y'][:][idx]\n",
    "Z = f['Z'][:][idx]\n",
    "f.close()\n",
    "\n",
    "for i in range(1,24):\n",
    "    if i%1 == 0:\n",
    "        !free -m\n",
    "    '''if i == 10:\n",
    "        continue'''\n",
    "    filename = r'C:\\Users\\hutom\\Downloads\\Compressed\\2018.01\\ExtractDataset\\part'+str(i) + '.h5'\n",
    "    print(filename)\n",
    "    f = h5py.File(filename,'r')\n",
    "    X = np.vstack((X,f['X'][:][idx]))\n",
    "    Y = np.vstack((Y,f['Y'][:][idx]))\n",
    "    Z = np.vstack((Z,f['Z'][:][idx]))\n",
    "    f.close()\n",
    "\n",
    "\n",
    "print('X-size：',X.shape)\n",
    "print('Y-size：',Y.shape)\n",
    "print('Z-size：',Z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (1152000, 1024, 2)\n",
      "Y_train: (1152000, 24)\n",
      "X_test: (288000, 1024, 2)\n",
      "Y_test: (288000, 24)\n"
     ]
    }
   ],
   "source": [
    "n_examples = X.shape[0]\n",
    "n_train = int(n_examples * 0.8)   \n",
    "train_idx = np.random.choice(range(0,n_examples), size=n_train, replace=False)  #Randomly select training sample subscript\n",
    "test_idx = list(set(range(0,n_examples))-set(train_idx)) #Test sample index\n",
    "X_train = X[train_idx]  #training samples\n",
    "X_test =  X[test_idx]  #testing samples\n",
    "Y_train = Y[train_idx]\n",
    "Y_test = Y[test_idx]\n",
    "print(\"X_train:\",X_train.shape)\n",
    "print(\"Y_train:\",Y_train.shape)\n",
    "print(\"X_test:\",X_test.shape)\n",
    "print(\"Y_test:\",Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 1024, 2)]         0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 2, 1024)           0         \n",
      "_________________________________________________________________\n",
      "transformer_block_1 (Transfo (None, 2, 1024)           4728064   \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_1 ( (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "alpha_dropout_3 (AlphaDropou (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 128)               131200    \n",
      "_________________________________________________________________\n",
      "alpha_dropout_4 (AlphaDropou (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "alpha_dropout_5 (AlphaDropou (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 24)                3096      \n",
      "=================================================================\n",
      "Total params: 4,882,968\n",
      "Trainable params: 4,880,920\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classes = ['32PSK',\n",
    " '16APSK',\n",
    " '32QAM',\n",
    " 'FM',\n",
    " 'GMSK',\n",
    " '32APSK',\n",
    " 'OQPSK',\n",
    " '8ASK',\n",
    " 'BPSK',\n",
    " '8PSK',\n",
    " 'AM-SSB-SC',\n",
    " '4ASK',\n",
    " '16PSK',\n",
    " '64APSK',\n",
    " '128QAM',\n",
    " '128APSK',\n",
    " 'AM-DSB-SC',\n",
    " 'AM-SSB-WC',\n",
    " '64QAM',\n",
    " 'QPSK',\n",
    " '256QAM',\n",
    " 'AM-DSB-WC',\n",
    " 'OOK',\n",
    " '16QAM']\n",
    "\n",
    "class MultiHeadSelfAttention(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads=8):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        if embed_dim % num_heads != 0:\n",
    "            raise ValueError(\n",
    "                f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\"\n",
    "            )\n",
    "        self.projection_dim = embed_dim // num_heads\n",
    "        self.query_dense = layers.Dense(embed_dim)\n",
    "        self.key_dense = layers.Dense(embed_dim)\n",
    "        self.value_dense = layers.Dense(embed_dim)\n",
    "        self.combine_heads = layers.Dense(embed_dim)\n",
    "    def attention(self, query, key, value):\n",
    "        score = tf.matmul(query, key, transpose_b=True)\n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "        scaled_score = score / tf.math.sqrt(dim_key)\n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
    "        output = tf.matmul(weights, value)\n",
    "        return output, weights \n",
    "    def separate_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # x.shape = [batch_size, seq_len, embedding_dim]\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        query = self.query_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        key = self.key_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        value = self.value_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        query = self.separate_heads(\n",
    "            query, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        key = self.separate_heads(\n",
    "            key, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        value = self.separate_heads(\n",
    "            value, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        attention, weights = self.attention(query, key, value)\n",
    "        attention = tf.transpose(\n",
    "            attention, perm=[0, 2, 1, 3]\n",
    "        )  # (batch_size, seq_len, num_heads, projection_dim)\n",
    "        concat_attention = tf.reshape(\n",
    "            attention, (batch_size, -1, self.embed_dim)\n",
    "        )  # (batch_size, seq_len, embed_dim)\n",
    "        output = self.combine_heads(\n",
    "            concat_attention\n",
    "        )  # (batch_size, seq_len, embed_dim)\n",
    "        return output\n",
    "    \n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.2):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.ffn = tf.keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "    def get_config(self):\n",
    "        cfg = super().get_config()\n",
    "#         cfg.update({\n",
    "#             'att': self.MultiHeadSelfAttention(1024, 128),\n",
    "#             'ffn': self.tf.keras.Sequential(\n",
    "#             [layers.Dense(256, activation=\"relu\"), layers.Dense(1024),]\n",
    "#         ),\n",
    "#             'layernorm1': self.layers.LayerNormalization(epsilon=1e-6),\n",
    "#             'layernorm2': self.layers.LayerNormalization(epsilon=1e-6),\n",
    "#             'dropout1': self.Dropout(0.2),\n",
    "#             'dropout2': self.Dropout(0.2),\n",
    "#         })\n",
    "        return cfg    \n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "    \n",
    "def proposed_model(X_train,classes):\n",
    "\n",
    "    embed_dim = 1024  # Embedding size for each token\n",
    "    num_heads = 128  # Number of attention heads\n",
    "    ff_dim = 256  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "#     inputs = layers.Input(shape=(maxlen,))\n",
    "#     embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "#     x = embedding_layer(inputs)\n",
    "#     mirrored_strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\",\"/gpu:1\",\"/gpu:2\"], cross_device_ops=tf.distribute.HierarchicalCopyAllReduce())\n",
    "#     with mirrored_strategy.scope():\n",
    "    in_shp = X_train.shape[1:]   #Dimensions of each sample\n",
    "\n",
    "    #input layer\n",
    "    X_input = Input(in_shp)\n",
    "    X = Reshape([2,1024], input_shape=in_shp)(X_input)\n",
    "\n",
    "    transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "    x = transformer_block(X)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.AlphaDropout(0.3)(x)\n",
    "    x = layers.Dense(128, activation=\"selu\")(x)\n",
    "    x = layers.AlphaDropout(0.2)(x)\n",
    "    x = layers.Dense(128, activation=\"selu\")(x)\n",
    "    x = layers.AlphaDropout(0.2)(x)\n",
    "    x = layers.Dense(24, activation=\"softmax\")(x)\n",
    "\n",
    "#         X = Dense(len(classes))(x)\n",
    "#         #SoftMax\n",
    "#         X = Activation('softmax')(x)\n",
    "    return tf.keras.models.Model(inputs=X_input, outputs=x)     \n",
    "    \n",
    "model_trafo = proposed_model(X_train,classes)    \n",
    "# from focal_loss import BinaryFocalLoss\n",
    "# model.compile(loss=tfa.losses.triplet_hard_loss, optimizer=tfa.optimizers.LazyAdam(),metrics=['accuracy'])\n",
    "model_trafo.compile(loss='categorical_crossentropy', optimizer=tfa.optimizers.LazyAdam(),metrics=['accuracy'])\n",
    "model_trafo.load_weights('trafo_model')\n",
    "model_trafo.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_resnet = load_model('resnet2_model.h5')\n",
    "# model_trafo = load_model('transformer_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-bb7366c182b7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# Plot confusion matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1024\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mtest_Y_hat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[0mconf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mconfnorm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues, labels=[]):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(labels))\n",
    "    plt.xticks(tick_marks, labels, rotation=45)\n",
    "    plt.yticks(tick_marks, labels)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "# Plot confusion matrix\n",
    "batch_size = 1024\n",
    "test_Y_hat = model.predict(X_test, batch_size=3000)\n",
    "conf = np.zeros([len(classes),len(classes)])\n",
    "confnorm = np.zeros([len(classes),len(classes)])\n",
    "for i in range(0,X_test.shape[0]):\n",
    "    j = list(Y_test[i,:]).index(1)\n",
    "    k = int(np.argmax(test_Y_hat[i,:]))\n",
    "    conf[j,k] = conf[j,k] + 1\n",
    "for i in range(0,len(classes)):\n",
    "    confnorm[i,:] = conf[i,:] / np.sum(conf[i,:])\n",
    "plot_confusion_matrix(confnorm, labels=classes)\n",
    "\n",
    "for i in range(len(confnorm)):\n",
    "    print(classes[i],confnorm[i,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-a6d505992867>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_test_snr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m#Correct category index\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;31m#Prediction category index\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mpre_Y_test_resnet\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mpre_Y_test_transformer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m             \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpre_Y_test_resnet\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;32melse\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpre_Y_test_transformer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "acc={}\n",
    "Z_test = Z[test_idx]\n",
    "Z_test = Z_test.reshape((len(Z_test)))\n",
    "SNRs = np.unique(Z_test)\n",
    "for snr in SNRs:\n",
    "    X_test_snr = X_test[Z_test==snr]\n",
    "    Y_test_snr = Y_test[Z_test==snr]\n",
    "    \n",
    "    pre_Y_test_resnet = model_resnet.predict(X_test_snr)\n",
    "    pre_Y_test_transformer = model_trafo.predict(X_test_snr)\n",
    "    \n",
    "    conf = np.zeros([len(classes),len(classes)])\n",
    "    confnorm = np.zeros([len(classes),len(classes)])\n",
    "    for i in range(0,X_test_snr.shape[0]):    #Test data volume under this signal-to-noise ratio\n",
    "        j = list(Y_test_snr[i,:]).index(1)   #Correct category index\n",
    "        #Prediction category index\n",
    "        if pre_Y_test_resnet > pre_Y_test_transformer:\n",
    "            k = int(np.argmax(pre_Y_test_resnet[i,:])) \n",
    "        else : k = int(np.argmax(pre_Y_test_transformer[i,:])) \n",
    "       \n",
    "        conf[j,k] = conf[j,k] + 1\n",
    "    for i in range(0,len(classes)):\n",
    "        confnorm[i,:] = conf[i,:] / np.sum(conf[i,:])\n",
    "   \n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(confnorm, labels=classes, title=\"ConvNet Confusion Matrix (SNR=%d)\"%(snr))\n",
    "    \n",
    "    cor = np.sum(np.diag(conf))\n",
    "    ncor = np.sum(conf) - cor\n",
    "    print (\"Overall Accuracy %s: \"%snr, cor / (cor+ncor))\n",
    "    acc[snr] = 1.0*cor/(cor+ncor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43641067"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_Y_test_resnet.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10408, 24)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_Y_test_transformer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(acc.keys(),acc.values())\n",
    "plt.ylabel('ACC')\n",
    "plt.xlabel('SNR')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
